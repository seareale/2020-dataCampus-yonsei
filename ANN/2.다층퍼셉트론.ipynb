{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.다층퍼셉트론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 다층 퍼셉트론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**단층 퍼셉트론의 한계**\n",
    "- 단층 퍼셉트론은 정확한 선형 결정 경계를 찾는 문제에 대한 모형이므로, 비선형 데이터에 대해서는 처리가 불가능하다.\n",
    "<img src=\"img/ch1_14.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. 다층 퍼셉트론 (MLP; Multi-Layer Perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단층 퍼셉트론을 여러 층으로 쌓아 올려서 만든 신경망\n",
    "- 입력층과 출력층 외에 1개 이상의 TLU 층으로 구성된다.\n",
    "- **은닉층 (hidden layer)** : 입력층과 출력층 사이에 존재한다.\n",
    "    - 은닉층의 모든 노드들은 하위층의 모든 입력을 받는다.\n",
    "    - 은닉층의 모든 노드들은 상위층의 모든 입력으로 출력을 내보낸다.\n",
    "    - 은닉층에도 편향 노드가 존재한다.\n",
    "    - 은닉층의 모든 노드들 역시 가중치가 할당되고 갱신된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**다층 퍼셉트론의 구성 예**\n",
    "- 입력층과 출력층, 1개 이상의 은닉층으로 구성된다.\n",
    "    - 아래의 예는 1개의 은닉층이 존재하는 다층 퍼셉트론이다.\n",
    "<img src=\"img/ch2_1.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. 심층 신경망 (DNN; Deep Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다층 퍼셉트론 구조에서 특히 은닉층이 2개 이상인 경우를 **심층 신경망(deep neural network)** 이라고 한다.\n",
    "- 심층 신경망 구조를 통해서 학습을 수행하는 것을 **심층 학습**, 즉 **딥 러닝(deep learning)** 이라고 한다.\n",
    "<img src=\"img/ch2_2.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 역전파 (Backpropagation) 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다층 퍼셉트론 학습을 위해서 오차를 감소시키는 알고리즘\n",
    "\n",
    "\n",
    "1. 정방향(입력층부터 시작하여 출력층까지)으로 각 층마다 출력을 계산한다.\n",
    "2. 역방향(출력층부터 시작하여 입력층까지)으로 각 층마다 각각의 연결이 오차에 기여한 정도를 계산한다.\n",
    "3. 계산된 오차가 감소하도록 각각의 연결마다 오차에 기여한만큼 가중치를 갱신한다.\n",
    "<img src=\"img/ch2_3.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ 실제로는 오차를 가중치로 미분한 경사 값을 적용한다. (= 경사하강법)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 다층 퍼셉트론에서의 활성화 함수 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단층 퍼셉트론에서 적용했던 계단 함수는 실제 경사를 계산(미분)할 수 없기 때문에 활용되지 않는다.\n",
    "- 오류 역전파를 위해서 미분 가능한 함수를 활성화 함수로 사용하며, 일반적으로 고려되는 함수들은 다음과 같다.\n",
    "    - 시그모이드 (로지스틱) 함수\n",
    "    - 쌍곡 탄젠트 함수\n",
    "    - ReLU 함수\n",
    "    - 소프트맥스 함수 (출력층 전용으로 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. 시그모이드(로지스틱) 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 미분 가능하고 0과 1 사이의 출력 값을 가지는 대표적인 활성화 함수 중 하나이다.\n",
    "- 깊은 은닉층의 구조에서 출력층과 거리가 멀어질수록 학습 효과가 나빠진다. (vanishing gradient)\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "<img src=\"img/ch2_4.png\" width=\"70%\">\n",
    "※ 가중치가 0으로 수렴할 가능성이 있음 = 뉴런이 사라짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. 쌍곡 탄젠트 (tanh) 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 출력 값이 -1과 1 사이인 시그모이드 곡선 형태의 함수이다.\n",
    "- 출력에서 멀리 떨어져 있는 은닉층에서 학습 효과의 감쇄현상이 다소 완화된다.\n",
    "$$tanh(x) = 2\\sigma(2x) - 1$$\n",
    "<img src=\"img/ch2_5.png\" width=\"70%\">  \n",
    "※ 시그모이드보다 느리게 진행 = 미분값이 0에 가까워지는 속도가 느림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. ReLU (Rectified Linear Unit) 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 값이 0 이하인 경우에 일괄적으로 0을 적용하는 연속 함수로서, 계산 속도가 빠르다는 장점이 있다.\n",
    "- 일반적으로 시그모이드 형태보다 학습 효과가 우수하다고 알려져 있다.\n",
    "$$f(x) = max(0, x)$$\n",
    "<img src=\"img/ch2_6.png\" width=\"70%\">  \n",
    "※ 최소치는 0이지만, 상한선이 없음 = 강한 자극을 신호가 강하도록       \n",
    "※ 미분이 불가능한 점이 있음 -> 임의로 미분값 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. 소프트맥스 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반적으로 출력층에서 사용되는 활성화 함수이다.\n",
    "- 함수 수식의 분모가 모든 클래스의 출력들을 반영한 형태이고 분자는 특정 클래스의 출력에 해당하기 때문에, 출력층의 전체 노드에 대해 하나의 활성화 함수로 적용한다.  \n",
    "$$\\sigma(z)_{j} = \\frac{e^{z_j}}{\\Sigma_{k=1}^{K}e^{z_k}}\\quad for\\;j=1,\\;..., K$$\n",
    "<img src=\"img/ch2_7.png\" width=\"70%\">   \n",
    "※ K = 클래스 개수, $e^{z_{k}}$ = 해당 클래스일 확률($e^{z_{k}}$들의 합 = 1)  \n",
    "※ $z_{k}$ = 이전 은닉층에서 나온 결과 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 다층 퍼셉트론 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력층\n",
    "    - 입력 데이터가 그대로 통과하는 층이므로, 입력층 노드의 개수는 입력 특성의 개수와 동일하게 자동으로 지정된다.\n",
    "- 출력층\n",
    "    - 출력은 클래스 분류 결과이므로, 출력층 노드의 개수는 분류 대상 클래스의 개수와 동일하게 자동으로 지정된다.\n",
    "- 은닉층\n",
    "    - 은닉층 1개에 존재하는 노드의 개수는 경험적으로 100 또는 200을 기준으로 상황에 따라 조절한다.\n",
    "    - 은닉층들의 층 수가 늘어난다고 해서 반드시 학습 효과가 향상되는 것은 아니며, 1~2개 층을 배치한 것만으로도 최대 성능을 나타내는 경우가 흔하다.\n",
    "    \n",
    "※ 옛날 이야기 -> vanishing gradient문제가 거의 해결되었고, 특성의 수가 너무 커짐 -> 은닉층이 많을 수록 성능이 좋아짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습) 사이킷런으로 다층 퍼셉트론 학습 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **neural_network** 모듈에 있는 **MLPClassifier**를 이용하여 다층 퍼셉트론 객체를 생성한다.\n",
    "    - 매개변수 **learning_rate_init**은 학습률 초기값이며, 기본값은 0.001이다.\n",
    "    - 매개변수 **max_iter**는 학습의 반복 횟수, 즉 에폭(epoch)이다. 기본값은 200이다.\n",
    "    - 매개변수 **activation**은 은닉층에서 적용할 활성화 함수의 이름이다. 'identity', 'logistic', 'tanh', 'relu' 중 선택할 수 있으며, 기본값은 ‘relu’이다.\n",
    "    - 매개변수 **hidden_layer_sizes**는 은닉층의 규모에 대한 튜플이다. 기본값은 (100,)이며, 1개의 은닉층이 있고 100개의 노드를 배치하겠다는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neural_network as nn\n",
    "\n",
    "ppn_clf = nn.MLPClassifier(learning_rate_init=0.1,\\\n",
    "                          max_iter=1000,\\\n",
    "                          activation='relu',\\\n",
    "                          hidden_layer_sizes=(50,40,30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ 학습 데이터는 연습용으로 다음과 같이 준비한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[1, 2], [5, 6], [2, 3], [6, 7], [3, 1], [7, 5]]\n",
    "y_train = [0, 1, 0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 퍼셉트론 객체에 대하여 **fit** 메소드를 이용하여 훈련한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ppn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 실행 객체 또는 분류 모형에 대하여 **predict** 메소드를 이용하여 예측을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [[1.5, 3.1], [4, 4.5], [6.7, 4.7]]\n",
    "y_test = [0, 0, 1]\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 분석 결과를 평가한다. (정확도)\n",
    "    - **metrics** 모듈에 있는 **accuracy_score** 함수 또는 분류모형에 대해 **score** 메소드를 이용하여 정확도를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.667\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as mt\n",
    "\n",
    "score = mt.accuracy_score(y_test, y_pred)\n",
    "print('정확도 : {:.3f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.667\n"
     ]
    }
   ],
   "source": [
    "score = clf.score(X_test, y_test)\n",
    "print('정확도 : {:.3f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습2) 붓꽃데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as d\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.metrics as mt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = d.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "ms.train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neural_network as nn\n",
    "\n",
    "ppn_clf = nn.MLPClassifier(learning_rate_init=0.1,\\\n",
    "                          max_iter=1000,\\\n",
    "                          activation='relu',\\\n",
    "                          hidden_layer_sizes=(100, 100, 20), \\\n",
    "                          random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ppn_clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.978\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as mt\n",
    "\n",
    "score = mt.accuracy_score(y_test, y_pred)\n",
    "print('정확도 : {:.3f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.978\n"
     ]
    }
   ],
   "source": [
    "score = clf.score(X_test, y_test)\n",
    "print('정확도 : {:.3f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0]\n",
      " [ 0 17  1]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "print(mt.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고) MLPClassifier 의 추가 매개변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLPClassifier의 추가 매개변수**\n",
    "\n",
    "- solver\n",
    "    - 학습 수행 과정에서 가중치를 최적화 하기 위해서 적용되는 알고리즘의 이름이다.\n",
    "    - 'lbfgs', 'sgd', 'adam' 중 하나를 선택할 수 있으며, 기본값은 'adam' 이다.\n",
    "    - 'sgd'와 'adam' 은 확률적 경사 하강법을 기반으로 동작하며 학습률과 가중치의 갱신을 수행한다.\n",
    "    - 일반적으로 'adam'이 가장 좋은 성능을 보이며, 데이터 규모가 크지 않을 때는 'lbfgs' 가 더 우수할 수 있다.\n",
    "\n",
    "    ※ sgd : 확률적 경사하강방법 (GD에는 배치, 확률, 미니배치가 있지만 sklearn에는 확률만 구현) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- alpha\n",
    "    - L2 규제를 적용할 때의 alpha 값으로, 기본값은 0.0001 이다.\n",
    "    - 따라서 이 값을 크게 하면 계수(가중치)를 0에 근접하게 조정하여 학습 모형의 복잡도를 감소시킬 수 있다.\n",
    "    \n",
    "    ※ L1 규제 : 일부를 0으로 만듬 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch_size\n",
    "    - 확률적 경사 하강법 기반으로 최적화를 수행할 때 미니배치의 크기이다.\n",
    "    - 기본 설정은 'auto' 로서, 200과 데이터 개수 중 더 작은 값으로 자동 할당된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **learning_rate**\n",
    "    - 초기에 지정한 학습률 learning_rate_init의 값을 유지하거나 변경하도록 지정하는 문자열이다.\n",
    "    - ‘constant’, ‘invscaling’, ‘adaptive’ 중 선택할 수 있고, 기본값은 ‘constant’이다.\n",
    "    - ‘constant’는 learning_rate_init의 값을 계속 유지한다.\n",
    "    - ‘adaptive’는 지속적으로 손실이 감소하도록 학습률 값을 변경한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tol\n",
    "    - 최적화 허용 손실(오차)의 값으로, 기본값은 1e-4이다.\n",
    "    - 매개변수 n_iter_no_change 횟수만큼 수행하는 동안 tol 값만큼 손실을 해소하지 못하면 학습을 종료한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n_iter_no_change\n",
    "    - 매개변수 tol 값만큼의 손실 감소가 발생하지 않을 때 수행 가능한 최대 에폭 수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- verbose\n",
    "    - 학습을 반복할 때마다 경과를 화면에 출력할 것인지를 결정하는 매개변수로, 기본값은 False이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLPClassifier의 전체 매개변수 이름과 역할은 사이킷런 웹사이트의  \n",
    "[Documentation]–[API]–[Neural network models]의 [neural_network.MLPClassifier]를 클릭하면 확인할 수 있다.\n",
    "\n",
    "\n",
    "※ 링크 : https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://seareale.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://seareale.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bitc7d172dfbcaf4c84b1b0b3dca7174a39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
