{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사 하강법 (GD; Gradient Descent)\n",
    "- 주어진 문제의 **비용 함수의 결과 값을 최소화** 하도록 반복하여 매개변수를 조정해 가는 최적화(optimization) 기법 중 하나\n",
    "- \"늦은 밤에 산 속에서 길을 잃었을 때 가장 좋은 방법은 매 위치에서 가장 가파른 **경사를 따라서 아래로 내려가는 것**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 변수 θ에 대해서 비용 함수의 현재 경사(gradient)를 계산하여 비용 함수의 값이 감소되는 방향으로 진행한다.\n",
    "- 결과적으로 경사가 0이 되면 비용 함수의 값은 최소값에 수렴하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 스텝은 매 계산마다 적용되는 이동 거리에 해당하며, **학습률(learning rate)**이라고 한다.\n",
    "    - 이 학습률이 너무 작은 경우에는 결과 값이 최소값에 수렴하기까지 계산 반복이 많이 발생한다.\n",
    "    - 반대로, 학습률이 너무 큰 경우에는 아예 반대쪽으로 건너 뛰어서 잘못된 곳으로 가게 될 수도 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 경사 하강법은 **전역 최적값(global optimum)**을 찾지 못하고 **지역 최적값(local optimum)**을 구하게 될 수도 있다는 문제가 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지역 최적값 = 8.95006461747015\n"
     ]
    }
   ],
   "source": [
    "x_old = 0\n",
    "x_new = 5\n",
    "\n",
    "learning_rate = 0.01\n",
    "precision = 0.000001\n",
    "\n",
    "def cost_function(x):\n",
    "    return 4+x**3 -9 *x**2\n",
    "\n",
    "while abs(x_new - x_old) > precision:\n",
    "    x_old = x_new\n",
    "    x_new = x_old - learning_rate * cost_function(x_old)\n",
    "    \n",
    "print(\"지역 최적값 =\", x_new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 경사 하강법의 구현 예시\n",
    "![image.png](img/ch4_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. 회귀 분석에서의 경사 하강법\n",
    "- 선형 회귀 분석에서의 비용 함수 RSS는 2차 함수이면서 볼록 함수(convex function)이므로, 별도의 지역 최소값이 없고 1개의 전역 최소값만 존재한다.\n",
    "- 따라서 선형 회귀에서는 경사 하강법을 이용하여 RSS의 전역 최소값을 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. 경사 하강법의 유형\n",
    "- 경사 하강법은 학습 데이터를 어떻게 할당하는가에 따라서 크게 3종류로 구분할 수 있다. \n",
    "  \n",
    "  \n",
    "1. 배치 경사 하강법 (batch gradient descent)\n",
    "    - 경사를 1회 계산하기 위해서 전체 학습 데이터를 사용\n",
    "2. 확률적 경사 하강법 (SGD; stochastic gradient descent)\n",
    "    - 경사를 1회 계산하기 위해서 1개의 학습 데이터를 사용\n",
    "3. 미니배치 경사 하강법 (mini-batch gradient descent)\n",
    "    - 경사를 1회 계산하기 위해서 일부 학습 데이터를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배치 (Batch)\n",
    "    - 1회의 경사 업데이트에 사용되는 데이터 집합\n",
    "    - 이 때 사용되는 데이터 집합의 개수를 배치 크기라고 한다. \n",
    "        - 예) 전체 데이터가 100개 있을 때 배치 크기가 20이면 :  \n",
    "            1개의 배치마다 20개의 데이터가 있다.  \n",
    "            배치는 총 5개가 있고, 경사는 5회 업데이트된다.  \n",
    "  \n",
    "  \n",
    "- 에폭 (Epoch)\n",
    "    - 전체 데이터들을 한 번 사용한 횟수, 즉 학습의 반복 횟수\n",
    "        - 예) 데이터 100개, 배치가 5개일 때 에폭이 1000이면 :  \n",
    "            학습이 총 1000회 수행된다.  \n",
    "            경사는 총 5000회 업데이트된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 경사 하강법은 **배치 크기와 적용**에 따라서 크게 3종류로 구분할 수 있다.\n",
    "    1. 배치 경사 하강법 (batch gradient descent)\n",
    "        - 1개의 배치에 전체 학습 데이터가 모두 들어간다.\n",
    "    2. 확률적 경사 하강법 (SGD; stochastic gradient descent)\n",
    "        - 1개의 배치에 임의의 학습 데이터 1개만 들어간다.\n",
    "        - 중복된 데이터도 뽑힐 수 있다.\n",
    "    3. 미니배치 경사 하강법 (mini-batch gradient descent)\n",
    "        - 1개의 배치에 임의의 학습 데이터 여러 개가 들어간다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/ch4_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearRegression() => OLS\n",
    "\n",
    "SGGRegressor => SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.1'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as d\n",
    "\n",
    "diab = d.load_diabetes()\n",
    "X = diab.data\n",
    "y = diab.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as ms\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "ms.train_test_split(X,y, test_size=0.3, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 3621.6577463442936\n",
      "R2 = 0.3813656405640853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryzen\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1208: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "s5     224.729\n",
       "bmi    215.477\n",
       "bp     179.408\n",
       "s4     132.603\n",
       "s6     118.209\n",
       "age     58.919\n",
       "s1      50.543\n",
       "s2      26.369\n",
       "sex    -21.258\n",
       "s3    -140.372\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.linear_model as lm\n",
    "\n",
    "# reg = lm.LinearRegression().fit(...)\n",
    "# 인자를 넣지 않아도 잘 동작한다.\n",
    "# SGDRegressor()는 인자를 넣어야 잘 동작한다.\n",
    "\n",
    "reg = lm.SGDRegressor().fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# 매 경사마다 임의의 자료 1개를 선택함\n",
    "\n",
    "print(\"MSE =\",mt.mean_squared_error(y_test, y_pred))\n",
    "print(\"R2 =\",mt.r2_score(y_test, y_pred))\n",
    "\n",
    "coefs = pd.Series(np.round(reg.coef_,3), index=diab.feature_names)\n",
    "coefs.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SGDRegressor in module sklearn.linear_model._stochastic_gradient object:\n",
      "\n",
      "class SGDRegressor(BaseSGDRegressor)\n",
      " |  SGDRegressor(loss='squared_loss', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, warm_start=False, average=False)\n",
      " |  \n",
      " |  Linear model fitted by minimizing a regularized empirical loss with SGD\n",
      " |  \n",
      " |  SGD stands for Stochastic Gradient Descent: the gradient of the loss is\n",
      " |  estimated each sample at a time and the model is updated along the way with\n",
      " |  a decreasing strength schedule (aka learning rate).\n",
      " |  \n",
      " |  The regularizer is a penalty added to the loss function that shrinks model\n",
      " |  parameters towards the zero vector using either the squared euclidean norm\n",
      " |  L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
      " |  parameter update crosses the 0.0 value because of the regularizer, the\n",
      " |  update is truncated to 0.0 to allow for learning sparse models and achieve\n",
      " |  online feature selection.\n",
      " |  \n",
      " |  This implementation works with data represented as dense numpy arrays of\n",
      " |  floating point values for the features.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <sgd>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  loss : str, default='squared_loss'\n",
      " |      The loss function to be used. The possible values are 'squared_loss',\n",
      " |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'\n",
      " |  \n",
      " |      The 'squared_loss' refers to the ordinary least squares fit.\n",
      " |      'huber' modifies 'squared_loss' to focus less on getting outliers\n",
      " |      correct by switching from squared to linear loss past a distance of\n",
      " |      epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is\n",
      " |      linear past that; this is the loss function used in SVR.\n",
      " |      'squared_epsilon_insensitive' is the same but becomes squared loss past\n",
      " |      a tolerance of epsilon.\n",
      " |  \n",
      " |      More details about the losses formulas can be found in the\n",
      " |      :ref:`User Guide <sgd_mathematical_formulation>`.\n",
      " |  \n",
      " |  penalty : {'l2', 'l1', 'elasticnet'}, default='l2'\n",
      " |      The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
      " |      which is the standard regularizer for linear SVM models. 'l1' and\n",
      " |      'elasticnet' might bring sparsity to the model (feature selection)\n",
      " |      not achievable with 'l2'.\n",
      " |  \n",
      " |  alpha : float, default=0.0001\n",
      " |      Constant that multiplies the regularization term. The higher the\n",
      " |      value, the stronger the regularization.\n",
      " |      Also used to compute the learning rate when set to `learning_rate` is\n",
      " |      set to 'optimal'.\n",
      " |  \n",
      " |  l1_ratio : float, default=0.15\n",
      " |      The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
      " |      l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
      " |      Only used if `penalty` is 'elasticnet'.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether the intercept should be estimated or not. If False, the\n",
      " |      data is assumed to be already centered.\n",
      " |  \n",
      " |  max_iter : int, default=1000\n",
      " |      The maximum number of passes over the training data (aka epochs).\n",
      " |      It only impacts the behavior in the ``fit`` method, and not the\n",
      " |      :meth:`partial_fit` method.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  tol : float, default=1e-3\n",
      " |      The stopping criterion. If it is not None, training will stop\n",
      " |      when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n",
      " |      epochs.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  shuffle : bool, default=True\n",
      " |      Whether or not the training data should be shuffled after each epoch.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      The verbosity level.\n",
      " |  \n",
      " |  epsilon : float, default=0.1\n",
      " |      Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
      " |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      " |      For 'huber', determines the threshold at which it becomes less\n",
      " |      important to get the prediction exactly right.\n",
      " |      For epsilon-insensitive, any differences between the current prediction\n",
      " |      and the correct label are ignored if they are less than this threshold.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used for shuffling the data, when ``shuffle`` is set to ``True``.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  learning_rate : string, default='invscaling'\n",
      " |      The learning rate schedule:\n",
      " |  \n",
      " |      - 'constant': `eta = eta0`\n",
      " |      - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n",
      " |        where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
      " |      - 'invscaling': `eta = eta0 / pow(t, power_t)`\n",
      " |      - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n",
      " |        Each time n_iter_no_change consecutive epochs fail to decrease the\n",
      " |        training loss by tol or fail to increase validation score by tol if\n",
      " |        early_stopping is True, the current learning rate is divided by 5.\n",
      " |  \n",
      " |          .. versionadded:: 0.20\n",
      " |              Added 'adaptive' option\n",
      " |  \n",
      " |  eta0 : double, default=0.01\n",
      " |      The initial learning rate for the 'constant', 'invscaling' or\n",
      " |      'adaptive' schedules. The default value is 0.01.\n",
      " |  \n",
      " |  power_t : double, default=0.25\n",
      " |      The exponent for inverse scaling learning rate.\n",
      " |  \n",
      " |  early_stopping : bool, default=False\n",
      " |      Whether to use early stopping to terminate training when validation\n",
      " |      score is not improving. If set to True, it will automatically set aside\n",
      " |      a fraction of training data as validation and terminate\n",
      " |      training when validation score returned by the `score` method is not\n",
      " |      improving by at least `tol` for `n_iter_no_change` consecutive\n",
      " |      epochs.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |          Added 'early_stopping' option\n",
      " |  \n",
      " |  validation_fraction : float, default=0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if `early_stopping` is True.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |          Added 'validation_fraction' option\n",
      " |  \n",
      " |  n_iter_no_change : int, default=5\n",
      " |      Number of iterations with no improvement to wait before early stopping.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |          Added 'n_iter_no_change' option\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      " |      result in a different solution than when calling fit a single time\n",
      " |      because of the way the data is shuffled.\n",
      " |      If a dynamic learning rate is used, the learning rate is adapted\n",
      " |      depending on the number of samples already seen. Calling ``fit`` resets\n",
      " |      this counter, while ``partial_fit``  will result in increasing the\n",
      " |      existing counter.\n",
      " |  \n",
      " |  average : bool or int, default=False\n",
      " |      When set to True, computes the averaged SGD weights accross all\n",
      " |      updates and stores the result in the ``coef_`` attribute. If set to\n",
      " |      an int greater than 1, averaging will begin once the total number of\n",
      " |      samples seen reaches `average`. So ``average=10`` will begin\n",
      " |      averaging after seeing 10 samples.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : ndarray of shape (n_features,)\n",
      " |      Weights assigned to the features.\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,)\n",
      " |      The intercept term.\n",
      " |  \n",
      " |  average_coef_ : ndarray of shape (n_features,)\n",
      " |      Averaged weights assigned to the features. Only available\n",
      " |      if ``average=True``.\n",
      " |  \n",
      " |      .. deprecated:: 0.23\n",
      " |          Attribute ``average_coef_`` was deprecated\n",
      " |          in version 0.23 and will be removed in 0.25.\n",
      " |  \n",
      " |  average_intercept_ : ndarray of shape (1,)\n",
      " |      The averaged intercept term. Only available if ``average=True``.\n",
      " |  \n",
      " |      .. deprecated:: 0.23\n",
      " |          Attribute ``average_intercept_`` was deprecated\n",
      " |          in version 0.23 and will be removed in 0.25.\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      The actual number of iterations before reaching the stopping criterion.\n",
      " |  \n",
      " |  t_ : int\n",
      " |      Number of weight updates performed during training.\n",
      " |      Same as ``(n_iter_ * n_samples)``.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.linear_model import SGDRegressor\n",
      " |  >>> from sklearn.pipeline import make_pipeline\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>> n_samples, n_features = 10, 5\n",
      " |  >>> rng = np.random.RandomState(0)\n",
      " |  >>> y = rng.randn(n_samples)\n",
      " |  >>> X = rng.randn(n_samples, n_features)\n",
      " |  >>> # Always scale the input. The most convenient way is to use a pipeline.\n",
      " |  >>> reg = make_pipeline(StandardScaler(),\n",
      " |  ...                     SGDRegressor(max_iter=1000, tol=1e-3))\n",
      " |  >>> reg.fit(X, y)\n",
      " |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      " |                  ('sgdregressor', SGDRegressor())])\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  Ridge, ElasticNet, Lasso, sklearn.svm.SVR\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SGDRegressor\n",
      " |      BaseSGDRegressor\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      BaseSGD\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, loss='squared_loss', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, warm_start=False, average=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseSGDRegressor:\n",
      " |  \n",
      " |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      " |      Fit linear model with Stochastic Gradient Descent.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Training data\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          Target values\n",
      " |      \n",
      " |      coef_init : ndarray of shape (n_features,), default=None\n",
      " |          The initial coefficients to warm-start the optimization.\n",
      " |      \n",
      " |      intercept_init : ndarray of shape (1,), default=None\n",
      " |          The initial intercept to warm-start the optimization.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  partial_fit(self, X, y, sample_weight=None)\n",
      " |      Perform one epoch of stochastic gradient descent on given samples.\n",
      " |      \n",
      " |      Internally, this method uses ``max_iter = 1``. Therefore, it is not\n",
      " |      guaranteed that a minimum of the cost function is reached after calling\n",
      " |      it once. Matters such as objective convergence and early stopping\n",
      " |      should be handled by the user.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Subset of training data\n",
      " |      \n",
      " |      y : numpy array of shape (n_samples,)\n",
      " |          Subset of target values\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples.\n",
      " |          If not provided, uniform weights are assumed.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ndarray of shape (n_samples,)\n",
      " |         Predicted target values per element in X.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from BaseSGDRegressor:\n",
      " |  \n",
      " |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a\n",
      " |          precomputed kernel matrix or a list of generic objects instead,\n",
      " |          shape = (n_samples, n_samples_fitted),\n",
      " |          where n_samples_fitted is the number of\n",
      " |          samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The R2 score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseSGD:\n",
      " |  \n",
      " |  set_params(self, **kwargs)\n",
      " |      Set and validate the parameters of estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseSGD:\n",
      " |  \n",
      " |  average_coef_\n",
      " |  \n",
      " |  average_intercept_\n",
      " |  \n",
      " |  standard_coef_\n",
      " |  \n",
      " |  standard_intercept_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lm.SGDRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 3445.9583281968708\n",
      "R2 = 0.4113777799244678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "s5     268.675\n",
       "bmi    259.381\n",
       "bp     215.643\n",
       "s4     144.615\n",
       "s6     129.105\n",
       "age     58.140\n",
       "s1      45.429\n",
       "s2      13.551\n",
       "sex    -41.138\n",
       "s3    -163.317\n",
       "dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = lm.SGDRegressor(max_iter=1000000, tol=0.0001, eta0 = 0.1).fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "print(\"MSE =\",mt.mean_squared_error(y_test, y_pred))\n",
    "print(\"R2 =\",mt.r2_score(y_test, y_pred))\n",
    "\n",
    "coefs = pd.Series(np.round(reg.coef_,3), index=diab.feature_names)\n",
    "coefs.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그때 그때 다르게 나온다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비용함수에 꺽인점이 있으면 안된다. 비용함수는 모든 점에서 미분가능해야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 경사 하강법은 배치 크기가 전체데이터, 배치는 1개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보폭 -> learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 사이즈 별로 처리하는 부담이 다름, 데이터가 많을수록 시간이 오래 걸림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 적으면 전체를 대표한다고 말하기 어렵다. -> 경사를 따라가는게 부드럽지 않을 수 있다 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "천 만 개 데이터를 배치 경사 하강법으로 : 배치 크기 천만 배치개수 1. 에폭 1 (경사 갱신 천만) \n",
    "확률적 경사 하강법으로 : 배치 크기 1, 배치개수 1000 . 에폭 천만. (경사 갱신 천만 * 천만) 이 맞나요..?! \n",
    "그래도 확률적 경사 하강법이 더 빠를까요?\n",
    "\n",
    "확률 경사가 더빠름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bitc7d172dfbcaf4c84b1b0b3dca7174a39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
